# SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases

This repository contains the reproducible artifacts for our VLDB 2026 submission: **"SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"**.

## Overview

SAM (Stability-Aware Manager) is a novel cache allocation strategy designed for multi-tenant embedded database systems. The core control policy of SAM is **AURA (Autonomic Utility-balancing Resource Allocator)**, which combines horizontal factors (current efficiency) with vertical factors (marginal gains) to achieve both performance and stability in cache allocation decisions.

## A Note on the Performance Metric: Effective Throughput

Our paper's core motivation is the high cost of cache misses in a network-bound Cloud-Edge environment. Reproducing this on a local, CPU-bound machine makes raw Transactions Per Second (TPS) an unreliable metric, as it is masked by the CPU bottleneck.

To provide a faithful evaluation, our artifact measures the raw **hit and miss counts** (which are accurate in any environment) and calculates a modeled **"Effective Throughput"**. This metric represents the performance that would be achieved in the target environment by applying a cost model where a cache miss is 50x more expensive than a cache hit.

**Therefore, all throughput plots generated by this artifact represent this "Effective Throughput", not raw measured TPS.** This is a principled approach to validate our core claims in a controlled setting. The "Effective Throughput" is calculated as: Total Transactions / (Total Hits * $T_{hit}$ + Total Misses * $T_{miss}$), where $T_{hit}$ = 1 and $T_{miss}$ = 50.

## Key Contributions

1. We propose **SAM**, a cache management system built on a novel, stability-first design principle. Its **two-pool architecture** is the first to systematically decouple baseline SLA guarantees from dynamic performance optimization, architecturally separating safety from efficiency.

2. We introduce a **principled engineering approach** to stability-aware control, embodied in our policy, **AURA**. It navigates the complex exploitation-exploration trade-off by practically synthesizing a tenant's historical efficiency (H-factor) with its forward-looking marginal gain (V-factor).

3. We provide extensive experimental validation of SAM's superiority and practical viability against 14 diverse baselines. Our results on standard, adversarial, and TPC-C workloads demonstrate near-optimal throughput, unique resilience, and excellent scalability.

## Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/SAM-reproducible.git
cd SAM-reproducible

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Running Experiments

Our artifact provides two ways to run experiments: a unified shell script for easy reproduction of paper figures, and direct Python commands for more detailed control.

#### Relationship Between Scripts and Experiments

The **run.sh** script is a user-friendly wrapper that maps **paper figure numbers** to the underlying experiment types:
- **[Paper Figure 3&4]** → Type 1 experiment (Performance comparison)
- **[Paper Figure 5]** → Type 2 experiment (Robustness test)  
- **[Paper Figure 9]** → Type 3 experiment (Scalability analysis)

This design allows reviewers to directly reproduce specific figures **as they appear in our submitted paper** without needing to understand the internal experiment numbering.

#### Quick Start: Reproducing Paper Figures

```bash
# Make sure virtual environment is activated before running

# Reproduce ALL figures from the paper (~95 minutes total)
./run.sh all         # or simply: ./run.sh

# Reproduce SPECIFIC figures from the paper
./run.sh fig34      # Paper Figure 3&4: Performance comparison (~45 minutes)
./run.sh fig5       # Paper Figure 5: Robustness test (~20 minutes)
./run.sh fig9       # Paper Figure 9: Scalability analysis (~30 minutes)

# Show usage information
./run.sh --help
```

#### Advanced: Direct Python Commands

For reviewers who want more control over the experiments or to understand the underlying implementation, the Python scripts can be called directly:

##### Type 1: Performance Comparison (generates **Paper Figure 3&4**)
```bash
# Compare SAM with baseline strategies (B2, B7, B12) under hotspot shift workload
# Output: figures/paper/figure1_performance_comparison.pdf (→ Paper Figure 3)
#         figures/paper/figure2_timeseries.pdf (→ Paper Figure 4)
python scripts/run_cache_strategy_comparison.py --experiment-type 1
```

##### Type 2: Robustness Test (generates **Paper Figure 5**)
```bash
# Test SAM and B7 against adversarial cache pollution attacks
# Output: figures/paper/figure7_dual_combat_robustness.pdf (→ Paper Figure 5)
python scripts/run_cache_strategy_comparison.py --experiment-type 2
```

##### Type 3: Scalability Analysis (generates **Paper Figure 9**)
```bash
# Evaluate SAM's scalability with 20, 40, 80, and 120 databases
# Output: figures/cpu_performance/cpu_performance_comparison.pdf (→ Paper Figure 9a)
#         figures/cpu_performance/cpu_decision_time_distribution.pdf (→ Paper Figure 9b)
python scripts/run_cache_strategy_comparison.py --experiment-type 3
```

**Note**: The shell script (`run.sh`) internally calls these Python commands with appropriate parameters, providing a more intuitive interface for reproducing specific paper figures.

### 3. Database Generation

**Important**: 
- All databases are generated automatically on first run
- No external data download required
- Data is generated in the `data/` directory

**Database Configuration**:
- **Type 1 & 2**: 10 databases (3 main + 7 background)
  - Main: 300K, 200K, 50K records
  - Background: 7 × 10K records
- **Type 3**: 20, 40, 80 and 120 databases
- **Total Cache**: 256 pages (1MB)
- **Generation Time**: ~10-15 minutes

## Repository Structure

```
SAM-reproducible/
├── src/                        # Source code
│   ├── strategies/             # Cache allocation strategies
│   │   ├── s0_strategy/        # SAM implementation (internally S0_EMG_AS)
│   │   └── baseline_strategies/# Baseline strategies (B1-B13)
│   └── experiment_manager.py  # Main experiment orchestrator
├── scripts/                    # Executable scripts
│   ├── run_cache_strategy_comparison.py  # Main experiment runner
│   ├── plot_dual_combat_story.py         # Type 2 visualization
│   └── plot_cpu_performance_comparison.py # Type 3 visualization
├── configs/                    # Configuration files
│   ├── config_tps_controlled_comparison.json  # Type 1 config
│   ├── config_dual_combat.json               # Type 2 config
│   ├── config_burst_scan_optimized.json      # Type 2 burst scan
│   └── config_scalability_template.json      # Type 3 template
├── docs/                       # Documentation (not tracked in git)
├── figures/                    # Generated figures (not tracked in git)
├── results/                    # Experiment results (not tracked in git)
└── data/                       # Database files (not tracked in git)
```

## Strategy Naming Convention

**Internal Implementation Name** → **Paper Name** → **Description**
- `S0_EMG_AS` → **SAM** → Our proposed Stability-Aware Manager with AURA control policy

### List of Implemented Strategies:
- `B1_StaticAverage` → **B1** → Static equal allocation
- `B2_NoElasticFixedByPriority` → **B2** → Priority-based static allocation  
- `B3_NoFixedElasticByPriority` → **B3** → Pure elastic allocation (SAM ablation)
- `B4_IndividualOptimizedCache` → **B4** → Independent optimization per database
- `B5_SimulatedGlobalLru` → **B5** → Global LRU simulation
- `B6_DataSizeProportionalStatic` → **B6** → Data size proportional static
- `B7_DYNAMIC_NEED` → **B7** → Dynamic need-based allocation
- `B8_EFFICIENCY_ONLY` → **B8** → H-factor only (SAM ablation)
- `B9_EMG_AS_SINGLE_EMA` → **B9** → Fast EMA only (SAM ablation)
- `B10_Pure_V_Factor` → **B10** → V-factor only (SAM ablation)
- `B11_ML_Driven` → **B11** → Machine learning based
- `B12_MT_LRU_Inspired` → **B12** → MT-LRU inspired SLA-based
- `B13_Active_Sampling` → **B13** → Active sampling strategy (UCP-inspired)

## Figure Mapping to Paper

The experiments generate figures that correspond **exactly** to those in our submitted paper:

### [Paper Figure 3&4] - Performance Comparison (Type 1 Experiment)
- **Paper Figure 3**: `figures/paper/figure1_performance_comparison.pdf` - Overall performance metrics
- **Paper Figure 4**: `figures/paper/figure2_timeseries.pdf` - Cache allocation over time

### [Paper Figure 5] - Robustness Test (Type 2 Experiment)
- **Paper Figure 5**: `figures/paper/figure7_dual_combat_robustness.pdf` - Three-panel analysis:
  - (a) B7's overreaction to burst scan attacks
  - (b) VIP customer performance under attack
  - (c) Resource misallocation comparison

### [Paper Figure 9] - Scalability Analysis (Type 3 Experiment)
- **Paper Figure 9a**: `figures/cpu_performance/cpu_performance_comparison.pdf` - CPU time vs database count
- **Paper Figure 9b**: `figures/cpu_performance/cpu_decision_time_distribution.pdf` - Decision time distribution

**Note for Reviewers**: These are the exact figures that appear in our VLDB 2026 submission. Running the experiments will reproduce them with identical layouts and results.

## Hardware Requirements

- **Minimum**: 4GB RAM, 10GB disk space
- **Recommended**: 8GB+ RAM, SSD storage
- **OS**: Linux, macOS, or Windows with Python 3.8+
- **Dependencies**: SQLite3 with APSW bindings

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.